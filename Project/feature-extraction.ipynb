{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a61b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b19ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1de9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset_structure(dataset_path):\n",
    "    \"\"\"\n",
    "    Explore and understand the TESS dataset organization\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the TESS dataset folder\n",
    "        \n",
    "    Learning Point:\n",
    "        - Understanding how data is organized is the FIRST step\n",
    "        - This helps us write code to load all files systematically\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"DATASET STRUCTURE EXPLORATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get all subdirectories (each represents actress + emotion combination)\n",
    "    folders = [f for f in os.listdir(dataset_path) \n",
    "               if os.path.isdir(os.path.join(dataset_path, f))]\n",
    "    \n",
    "    print(f\"\\nüìÅ Total folders found: {len(folders)}\")\n",
    "    print(f\"\\nFolder names (Actress_Emotion format):\")\n",
    "    for folder in sorted(folders):\n",
    "        # Count files in each folder\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        wav_files = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "        print(f\"  - {folder}: {len(wav_files)} files\")\n",
    "    \n",
    "    # Extract unique actresses and emotions\n",
    "    actresses = set([f.split('_')[0] for f in folders])\n",
    "    emotions = set([f.split('_')[1] for f in folders])\n",
    "    \n",
    "    print(f\"\\nüë§ Actresses: {sorted(actresses)}\")\n",
    "    print(f\"üòä Emotions: {sorted(emotions)}\")\n",
    "    \n",
    "    return folders, sorted(actresses), sorted(emotions)\n",
    "\n",
    "def create_dataset_dataframe(dataset_path):\n",
    "    \"\"\"\n",
    "    Create a pandas DataFrame with all audio file information\n",
    "    \n",
    "    Learning Point:\n",
    "        - DataFrames help organize metadata\n",
    "        - Makes it easy to analyze distribution and select files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: file_path, actress, emotion, word\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CREATING DATASET INVENTORY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Iterate through all folders\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        \n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "            \n",
    "        # Parse folder name: e.g., \"OAF_angry\" -> actress=OAF, emotion=angry\n",
    "        parts = folder.split('_')\n",
    "        actress = parts[0]\n",
    "        emotion = parts[1]\n",
    "        \n",
    "        # Get all WAV files in this folder\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.wav'):\n",
    "                # Example filename: \"OAF_angry_back.wav\"\n",
    "                # Extract the word (last part before .wav)\n",
    "                word = file.replace('.wav', '').split('_')[-1]\n",
    "                \n",
    "                data.append({\n",
    "                    'file_path': os.path.join(folder_path, file),\n",
    "                    'actress': actress,\n",
    "                    'emotion': emotion,\n",
    "                    'word': word,\n",
    "                    'filename': file\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset DataFrame created!\")\n",
    "    print(f\"   Total audio files: {len(df)}\")\n",
    "    print(f\"\\nFirst few entries:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Show distribution\n",
    "    print(f\"\\nüìä Distribution by Emotion:\")\n",
    "    print(df['emotion'].value_counts().sort_index())\n",
    "    \n",
    "    print(f\"\\nüìä Distribution by Actress:\")\n",
    "    print(df['actress'].value_counts())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a94aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_audio(file_path):\n",
    "    \"\"\"\n",
    "    Load an audio file and extract basic information\n",
    "    \n",
    "    Learning Points:\n",
    "        - Audio is stored as an array of amplitude values\n",
    "        - Sample rate determines time resolution\n",
    "        - Duration = len(audio) / sample_rate\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to WAV file\n",
    "        \n",
    "    Returns:\n",
    "        audio: numpy array of audio samples\n",
    "        sr: sample rate (samples per second)\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    # sr=None means keep original sample rate\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Calculate basic properties\n",
    "    duration = len(audio) / sr\n",
    "    \n",
    "    print(f\"\\nüéµ Audio File Analysis:\")\n",
    "    print(f\"   File: {os.path.basename(file_path)}\")\n",
    "    print(f\"   Sample Rate: {sr} Hz (samples per second)\")\n",
    "    print(f\"   Total Samples: {len(audio)}\")\n",
    "    print(f\"   Duration: {duration:.2f} seconds\")\n",
    "    print(f\"   Amplitude Range: [{audio.min():.4f}, {audio.max():.4f}]\")\n",
    "    print(f\"   Shape: {audio.shape}\")\n",
    "    \n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def visualize_audio_waveform(audio, sr, emotion, word):\n",
    "    \"\"\"\n",
    "    Visualize the audio waveform (time domain representation)\n",
    "    \n",
    "    Learning Point:\n",
    "        - Waveform shows amplitude over time\n",
    "        - X-axis: time, Y-axis: amplitude\n",
    "        - You can SEE differences in emotions!\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    \n",
    "    # Create time axis\n",
    "    time = np.linspace(0, len(audio)/sr, len(audio))\n",
    "    \n",
    "    plt.plot(time, audio, linewidth=0.5)\n",
    "    plt.xlabel('Time (seconds)', fontsize=12)\n",
    "    plt.ylabel('Amplitude', fontsize=12)\n",
    "    plt.title(f'Waveform: {emotion.upper()} - \"{word}\"', fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add some annotations\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° What you're seeing:\")\n",
    "    print(\"   - The wave shows how sound pressure changes over time\")\n",
    "    print(\"   - Higher peaks = louder moments\")\n",
    "    print(\"   - Rapid oscillations = higher pitch\")\n",
    "    print(\"   - Notice how different emotions have different patterns!\")\n",
    "\n",
    "\n",
    "def visualize_spectrogram(audio, sr, emotion, word):\n",
    "    \"\"\"\n",
    "    Visualize the spectrogram (time-frequency representation)\n",
    "    \n",
    "    Learning Point:\n",
    "        - Shows WHICH frequencies are present at WHICH times\n",
    "        - Y-axis: frequency (pitch), X-axis: time, Color: intensity\n",
    "        - This is what we'll feed to neural networks!\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(audio)), ref=np.max)\n",
    "    \n",
    "    # Display\n",
    "    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', cmap='viridis')\n",
    "    plt.colorbar(format='%+2.0f dB', label='Intensity (dB)')\n",
    "    plt.title(f'Spectrogram: {emotion.upper()} - \"{word}\"', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Time (seconds)', fontsize=12)\n",
    "    plt.ylabel('Frequency (Hz)', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° What you're seeing:\")\n",
    "    print(\"   - Bright areas = strong frequencies at that time\")\n",
    "    print(\"   - Horizontal bands = sustained pitches\")\n",
    "    print(\"   - Different emotions create different patterns!\")\n",
    "    print(\"   - Neural networks will learn to recognize these patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_emotions_waveforms(df, emotions_to_compare, word=\"back\"):\n",
    "    \"\"\"\n",
    "    Compare waveforms of different emotions for the same word\n",
    "    \n",
    "    Learning Point:\n",
    "        - Visual comparison helps understand emotion differences\n",
    "        - Same word, different emotion = different audio patterns\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(len(emotions_to_compare), 1, figsize=(14, 3*len(emotions_to_compare)))\n",
    "    \n",
    "    if len(emotions_to_compare) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, emotion in enumerate(emotions_to_compare):\n",
    "        # Find a file with this emotion and word\n",
    "        sample = df[(df['emotion'] == emotion) & (df['word'] == word)]\n",
    "        \n",
    "        if len(sample) > 0:\n",
    "            file_path = sample.iloc[0]['file_path']\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "            \n",
    "            time = np.linspace(0, len(audio)/sr, len(audio))\n",
    "            \n",
    "            axes[idx].plot(time, audio, linewidth=0.5)\n",
    "            axes[idx].set_title(f'{emotion.upper()} - \"{word}\"', fontsize=12, fontweight='bold')\n",
    "            axes[idx].set_xlabel('Time (seconds)')\n",
    "            axes[idx].set_ylabel('Amplitude')\n",
    "            axes[idx].grid(True, alpha=0.3)\n",
    "            axes[idx].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Notice the differences:\")\n",
    "    print(\"   - Energy levels (amplitude)\")\n",
    "    print(\"   - Duration variations\")\n",
    "    print(\"   - Pattern complexity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f8d51c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéì\n",
      "TESS EMOTION RECOGNITION - LEARNING JOURNEY BEGINS!\n",
      "üéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéìüéì\n",
      "\n",
      "üìå IMPORTANT: Update the dataset_path variable below!\n",
      "======================================================================\n",
      "DATASET STRUCTURE EXPLORATION\n",
      "======================================================================\n",
      "\n",
      "üìÅ Total folders found: 14\n",
      "\n",
      "Folder names (Actress_Emotion format):\n",
      "  - OAF_angry: 200 files\n",
      "  - OAF_disgust: 200 files\n",
      "  - OAF_fear: 200 files\n",
      "  - OAF_happy: 200 files\n",
      "  - OAF_neutral: 200 files\n",
      "  - OAF_pleasant_surprise: 200 files\n",
      "  - OAF_sad: 200 files\n",
      "  - YAF_angry: 200 files\n",
      "  - YAF_disgust: 200 files\n",
      "  - YAF_fear: 200 files\n",
      "  - YAF_happy: 200 files\n",
      "  - YAF_neutral: 200 files\n",
      "  - YAF_pleasant_surprised: 200 files\n",
      "  - YAF_sad: 200 files\n",
      "\n",
      "üë§ Actresses: ['OAF', 'YAF']\n",
      "üòä Emotions: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'pleasant', 'sad']\n",
      "\n",
      "======================================================================\n",
      "CREATING DATASET INVENTORY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Dataset DataFrame created!\n",
      "   Total audio files: 2800\n",
      "\n",
      "First few entries:\n",
      "                                           file_path actress  emotion  \\\n",
      "0  /home/umairimran/OLD DISK/Univeristy/7th Semes...     YAF  disgust   \n",
      "1  /home/umairimran/OLD DISK/Univeristy/7th Semes...     YAF  disgust   \n",
      "2  /home/umairimran/OLD DISK/Univeristy/7th Semes...     YAF  disgust   \n",
      "3  /home/umairimran/OLD DISK/Univeristy/7th Semes...     YAF  disgust   \n",
      "4  /home/umairimran/OLD DISK/Univeristy/7th Semes...     YAF  disgust   \n",
      "\n",
      "      word               filename  \n",
      "0  disgust   YAF_cool_disgust.wav  \n",
      "1  disgust   YAF_kill_disgust.wav  \n",
      "2  disgust  YAF_shall_disgust.wav  \n",
      "3  disgust   YAF_back_disgust.wav  \n",
      "4  disgust   YAF_came_disgust.wav  \n",
      "\n",
      "üìä Distribution by Emotion:\n",
      "emotion\n",
      "angry       400\n",
      "disgust     400\n",
      "fear        400\n",
      "happy       400\n",
      "neutral     400\n",
      "pleasant    400\n",
      "sad         400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Distribution by Actress:\n",
      "actress\n",
      "YAF    1400\n",
      "OAF    1400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "ANALYZING SAMPLE AUDIO FILE\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'coverage.types' has no attribute 'Tracer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANALYZING SAMPLE AUDIO FILE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_analyze_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# STEP 5: Visualize waveform\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müìä Generating Waveform Visualization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mload_and_analyze_audio\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mLoad an audio file and extract basic information\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    sr: sample rate (samples per second)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Load audio file\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# sr=None means keep original sample rate\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(file_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate basic properties\u001b[39;00m\n\u001b[1;32m     22\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(audio) \u001b[38;5;241m/\u001b[39m sr\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/lazy_loader/__init__.py:83\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     81\u001b[0m submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m submod \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(submod_path)\n\u001b[0;32m---> 83\u001b[0m attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m attr_to_modules[name]:\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/lazy_loader/__init__.py:82\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[1;32m     81\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 82\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/librosa/core/audio.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoxr\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlazy\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit, stencil, guvectorize\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_fftlib\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frames_to_samples, time_to_samples\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/__init__.py:92\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Re-export decorators\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (cfunc, jit, njit, stencil,\n\u001b[1;32m     93\u001b[0m                                    jit_module)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Re-export vectorize decorators and the thread layer querying function\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mufunc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (vectorize, guvectorize, threading_layer,\n\u001b[1;32m     97\u001b[0m                             get_num_threads, set_num_threads,\n\u001b[1;32m     98\u001b[0m                             set_parallel_chunksize, get_parallel_chunksize,\n\u001b[1;32m     99\u001b[0m                             get_thread_id)\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/core/decorators.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MappingProxyType\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeprecationError, NumbaDeprecationWarning\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstencils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstencil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stencil\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, extending, sigutils, registry\n\u001b[1;32m     16\u001b[0m _logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/stencils/stencil.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllvmlite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ir \u001b[38;5;28;01mas\u001b[39;00m lir\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, typing, utils, ir, config, ir_utils, registry\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemplates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (CallableTemplate, signature,\n\u001b[1;32m     13\u001b[0m                                          infer_global, AbstractTemplate)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lower_builtin\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/core/ir_utils.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextending\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Intrinsic\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, typing, ir, analysis, postproc, rewrites, config\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtemplates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/core/extending.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models   \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_default \u001b[38;5;28;01mas\u001b[39;00m register_model  \u001b[38;5;66;03m# noqa: F401, E501\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpythonapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m box, unbox, reflect, NativeValue  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_helperlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _import_cython_function  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceMixin\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/core/pythonapi.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _helperlib\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     types, utils, config, lowering, cgutils, imputils, serialize,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PYVERSION\n\u001b[1;32m     17\u001b[0m PY_UNICODE_1BYTE_KIND \u001b[38;5;241m=\u001b[39m _helperlib\u001b[38;5;241m.\u001b[39mpy_unicode_1byte_kind\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/core/lowering.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_use_defs, must_use_alloca\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfirstlinefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_func_body_first_lineno\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoverage_support\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_registered_loc_notify\n\u001b[1;32m     22\u001b[0m _VarArgItem \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_VarArgItem\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvararg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseLower\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[0;32m~/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/.venv/lib/python3.12/site-packages/numba/misc/coverage_support.py:114\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m NotifyCompilerCoverage(col)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m coverage_available:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;129m@dataclass\u001b[39m(kw_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNumbaTracer\u001b[39;00m(\u001b[43mcoverage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTracer\u001b[49m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        Not actually a tracer as in the coverage implementation, which will\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m        setup a Python trace function. This implementation pretends to trace\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m        See coverage.PyTracer\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m         data: coverage\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mTTraceData\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'coverage.types' has no attribute 'Tracer'"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"üéì\"*35)\n",
    "print(\"TESS EMOTION RECOGNITION - LEARNING JOURNEY BEGINS!\")\n",
    "print(\"üéì\"*35)\n",
    "    \n",
    "    # STEP 1: SET YOUR DATASET PATH\n",
    "print(\"\\nüìå IMPORTANT: Update the dataset_path variable below!\")\n",
    "dataset_path = \"/home/umairimran/OLD DISK/Univeristy/7th Semester/Intro to NLP/nlp_lab/data/TESS-data\"\n",
    "    \n",
    "    # Example: dataset_path = \"/content/TESS\" or \"C:/Users/YourName/TESS\"\n",
    "    \n",
    "    # Check if path exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"\\n‚ùå Dataset path not found: {dataset_path}\")\n",
    "    print(\"Please download the dataset and update the path above.\")\n",
    "else:\n",
    "        # STEP 2: Explore dataset structure\n",
    "    folders, actresses, emotions = explore_dataset_structure(dataset_path)\n",
    "        \n",
    "        # STEP 3: Create organized DataFrame\n",
    "    df = create_dataset_dataframe(dataset_path)\n",
    "        \n",
    "        # STEP 4: Analyze a single audio file\n",
    "    sample_file = df.iloc[0]['file_path']\n",
    "    sample_emotion = df.iloc[0]['emotion']\n",
    "    sample_word = df.iloc[0]['word']\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYZING SAMPLE AUDIO FILE\")\n",
    "    print(\"=\"*70)\n",
    "        \n",
    "    audio, sr = load_and_analyze_audio(sample_file)\n",
    "        \n",
    "        # STEP 5: Visualize waveform\n",
    "    print(\"\\nüìä Generating Waveform Visualization...\")\n",
    "    visualize_audio_waveform(audio, sr, sample_emotion, sample_word)\n",
    "        \n",
    "        # STEP 6: Visualize spectrogram\n",
    "    print(\"\\nüìä Generating Spectrogram Visualization...\")\n",
    "    visualize_spectrogram(audio, sr, sample_emotion, sample_word)\n",
    "        \n",
    "        # STEP 7: Compare different emotions\n",
    "    print(\"\\nüìä Comparing Different Emotions...\")\n",
    "    emotions_to_compare = ['angry', 'happy', 'sad']  # Choose 3 emotions\n",
    "    compare_emotions_waveforms(df, emotions_to_compare, word=\"back\")\n",
    "        \n",
    "    print(\"\\n\" + \"‚úÖ\"*35)\n",
    "    print(\"STEP 1 COMPLETE! You now understand:\")\n",
    "    print(\"  ‚úì Dataset structure\")\n",
    "    print(\"  ‚úì How audio is represented as data\")\n",
    "    print(\"  ‚úì Waveforms vs Spectrograms\")\n",
    "    print(\"  ‚úì How emotions differ in audio signals\")\n",
    "    print(\"‚úÖ\"*35)\n",
    "        \n",
    "    print(\"\\nüéØ NEXT STEP: Feature Extraction!\")\n",
    "    print(\"   We'll extract MFCCs, Mel spectrograms, and more!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
